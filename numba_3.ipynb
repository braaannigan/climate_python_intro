{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just-In-Time compilers can give significant speed-ups over vectorised calculations, but you should be aware of potential pitfalls with this rapidly developing technology.\n",
    "\n",
    "## Thou shalt not loop in python, most of the time\n",
    "One of the most useful things you can learn as you begin as a\n",
    "scientific programmer in python\n",
    "is to avoid writing loops to perform calculations.  These loops are known to\n",
    "be **so much slower** than equivalent *vectorised* calculations.  For example,\n",
    "compare this simple sum of two matrices as a loop versus a vectorised\n",
    "operation using the + operator.\n",
    "\n",
    "First we define two input arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.random.rand(1000000)\n",
    "data2 = np.random.rand(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a looping function as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_sum( data, data2 ):\n",
    "    sum_array = np.empty_like( data )\n",
    "    for idx in np.arange( len( data ) ):\n",
    "        sum_array[idx] = data[idx] + data2[idx]\n",
    "    return sum_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while a vectorised addition function would be:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorised_sum( data, data2 ):\n",
    "    return data + data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the loop on my machine it takes:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353 ms ± 4.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loop_sum( data, data2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the vectorised function takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 ms ± 62 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vectorised_sum( data, data2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the vectorised calculation is about 300 times faster.\n",
    "\n",
    "The reason for this faster performance [has been explained before:](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/)\n",
    "python needs to do a lot less type checking when working with arrays rather than individual values. In addition, the data in an array is always stored in a contiguous block in memory, whereas the underlying data in other structures such as lists might be scatted around the memory.\n",
    "\n",
    "## Is there a role for looping then?\n",
    "One problem with this focus on vectorised operations is that when the calculation is complicated, it is often easier to write loops than the equivalent\n",
    "vectorised operation.  Indeed, when I'm figuring out a calculation on\n",
    "a grid, I often work it out on paper in a loop format and then have to\n",
    "convert this loop to a vectorised operation.  This extra step can be a problem, as\n",
    "doing the vectorisation often gives me a headache when figuring out\n",
    "edge cases such as dealing with coastlines in numerical models.\n",
    "\n",
    "One way to stick with the looping option is to use the\n",
    "[Numba](http://Numba.pydata.org) package. You then write your loop\n",
    "function as before, but this time add the *decorator* ```@jit``` to the start of your function:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def numba_loop_sum( data, data2 ):\n",
    "    sum_array = np.empty_like( data )\n",
    "    for idx in np.arange( len( data ) ):\n",
    "        sum_array[idx] = data[idx] + data2[idx]\n",
    "    return sum_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding ```@jit``` as a decorator your function\n",
    "is \"Just-In-Time\" compiled into fast machine code when you call the function. We can compare the performance of this operation with Numba to the vectorised function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.89 ms ± 35.8 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numba_loop_sum( data, data2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But wasn't that slower than the vectorised calculation?\n",
    "Ok, so the numba calculation was a lot slower in this case, so it may be worth the extra work to do the vectorisation.  However, let's compare a more complicated case that is more similar to the problem I was working on this week.\n",
    "\n",
    "The vectorised version is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complicated_vectorised_calc( data, data2 ):\n",
    "    total = (data**2 * data2**2)**0.5 + data**3 + data2**4\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the corresponding Numba loop is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def Numba_loop_calc( data, data2 ):\n",
    "    total = np.empty_like( data )\n",
    "    for idx in np.arange( len( data ) ):\n",
    "        total[idx] = (data[idx]**2 * data2[idx]**2)**0.5 + data[idx]**3 + data2[idx]**4\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance for this more complicated case, we find that for the vectorised calculation it takes about 122 ms per calculation, while the Numba calculation takes 4 ms. So in this case Numba gives a speed up of 30 times.\n",
    "\n",
    "## What's going on with that?\n",
    "A major selling point of NumPy is that it compiles and carries out your calculations in C. As such, you might expect it to offer similar performance to a compiled function decorated with ```@jit```, rather than being 30 times slower.  \n",
    "\n",
    "The reason for this slowdown, is that NumPy may not be doing exactly when you think it is doing in the compiled layer.  For the complicated calculation, you might guess than NumPy is doing something like the loop in ```Numba_loop_calc``` with all the type declarations and so on that you would expect in C.\n",
    "\n",
    "However, NumPy is actually doing a series of sub-loops for each operation in the overall loop and this sub-looping introduces extra computational overhead.  NumPy also needs to create at least one temporary array to hold the output of the sub-loops that have already been done, and so NumPy requires more memory and more calls to memory.  This is all explained by Nathaniel Smith (a person who knows a lot more about NumPy than I do!) in [this video. The whole talk is worth watching, but the part setting out the part on JIT compilers starts at 26 minutes](https://www.youtube.com/watch?v=fowHwlpGb34)\n",
    "\n",
    "What all this means is that as your calculation becomes more complicated, the attraction of writing a loop decorated with ```@jit``` increases. The JIT function is particularly attractive  if the creation of temporary arrays means that your calculation is causing memory errors.\n",
    "\n",
    "## Great, so there's no downsides.\n",
    "Unfortunately, there is a catch.  The main one is that the automatic compilation built into Numba is very complicated, so for most users it's not going to be very clear what is happening or whether something is silently going wrong.  If you do use Numba, then it might be a good time to [write a test function](http://swcarpentry.github.io/python-novice-inflammation/08-defensive/) to make sure your output is what you expect.\n",
    "\n",
    "A further issue is that the tech in automatic compilation is still rapidly maturing.  It would be reasonable to expect Numba to continue evolving over the coming years.  This can mean that the API changes or even that changes to the underlying algorithm mean the output you get changes. On account of this, I'd be wary of building Numba into a widely-used library unless you can be sure it will continue to be supported.\n",
    "\n",
    "The output of the NumPy function and the Numba function also differ for some values in the output arrays on my machine.  These differences are small (order 1e-16), but nonetheless exist.\n",
    "\n",
    "## So, where are we headed?\n",
    "The rapid development in Just-In-Time compilation has raised an interesting question discussed by Nathaniel Smith in the above video: should NumPy be developed to take advantage of this capability?\n",
    "\n",
    "The problem is that the JIT compilers can't see down to the C library that underlies NumPy. Instead, Numba works with NumPy because the developers of Numba have gone through each of the NumPy functions that Numba supports and developed compiled code for the functions.  This isn't a long-term solution for many reasons.  For example, every time the underlying NumPy function changes, someone will have to make sure Numba is changed accordingly.  It also means you'd have to start from scratch if you want to start using a different kernel such as PyTorch or TensorFlow.\n",
    "\n",
    "In the meantime, it's good to be aware of the strengths and weaknesses of JIT compilers for optimising your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python work3",
   "language": "python",
   "name": "work3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
